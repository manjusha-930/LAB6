import numpy as np
import pandas as pd
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# Load dataset
file_path = "/mnt/data/DCT_withoutduplicate 6 1 1.csv"
df = pd.read_csv(file_path)

# Separate features and target
X = df.drop(columns=["LABEL"]).values
y = df["LABEL"].values

# A1: Function to calculate entropy
def entropy(y):
    counts = np.bincount(y)
    probabilities = counts / len(y)
    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])

entropy_value = entropy(y)
print(f"Entropy: {entropy_value}")

# A2: Function to calculate Gini index
def gini_index(y):
    counts = np.bincount(y)
    probabilities = counts / len(y)
    return 1 - np.sum([p**2 for p in probabilities])

gini_value = gini_index(y)
print(f"Gini Index: {gini_value}")

# A3: Function to determine the best feature to split using Information Gain
def best_feature_to_split(X, y):
    base_entropy = entropy(y)
    info_gains = []
    for feature in range(X.shape[1]):
        values = np.unique(X[:, feature])
        weighted_entropy = sum([(np.sum(X[:, feature] == v) / len(y)) * entropy(y[X[:, feature] == v]) for v in values])
        info_gains.append(base_entropy - weighted_entropy)
    return np.argmax(info_gains)

best_feature = best_feature_to_split(X, y)
print(f"Best feature to split: {best_feature}")

# A4: Function for binning continuous features
def bin_data(X, n_bins=4, strategy='uniform'):
    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)
    return binner.fit_transform(X.reshape(-1, 1)).astype(int)

# Example binning on first feature
binned_feature = bin_data(X[:, 0])
print(f"Binned Feature Sample: {binned_feature[:10].flatten()}")

# A5: Build Decision Tree model
def build_decision_tree(X, y):
    dt = DecisionTreeClassifier(criterion='entropy')
    dt.fit(X, y)
    return dt

dt_model = build_decision_tree(X, y)

# A6: Visualize Decision Tree
def visualize_decision_tree(model, feature_names):
    plt.figure(figsize=(12, 8))
    plot_tree(model, filled=True, feature_names=feature_names)
    plt.show()

feature_names = [f'Feature {i}' for i in range(X.shape[1])]
visualize_decision_tree(dt_model, feature_names)

# A7: Plot decision boundary
def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')
    plt.show()

# Execute decision boundary visualization
if X.shape[1] >= 2:
    plot_decision_boundary(X[:, :2], y, dt_model)
